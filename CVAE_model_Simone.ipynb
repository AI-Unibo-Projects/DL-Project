{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX-cm9oaC1RG",
        "colab_type": "text"
      },
      "source": [
        "# Project \\#3 - Face generation\n",
        "\n",
        "### Deep Learning course -  A.Y. 2019-2020\n",
        "\n",
        "Students:\n",
        "- Simone Gayed Said\n",
        "- Pierpasquale Colagrande\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GihxzxbDldj",
        "colab_type": "text"
      },
      "source": [
        "## Import of fundamental libraries\n",
        "Herw we import fundamental libraries as TensorFlow, Numpy etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDO_HonUBI7x",
        "colab_type": "code",
        "outputId": "8b344ed9-32f7-4406-8437-26f70e829689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HM5aL38Crbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Input, Flatten, Dense, Reshape, Concatenate, Lambda, Concatenate, Layer, Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import metrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTf8kEYfxIf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Input pipeline\n",
        "\n",
        "Our input data is stored on Google Cloud Storage. To more fully use the parallelism TPUs offer us, and to avoid bottlenecking on data transfer, we've stored our input data in TFRecord files, 2025 images per file.\n",
        "\n",
        "Below, we make heavy use of `tf.data.experimental.AUTOTUNE` to optimize different parts of input loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEHyAVAoFyu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 60000\n",
        "ORIGINAL_IMAGE_SIZE = (218, 178, 3)\n",
        "TARGET_IMAGE_SIZE = (64, 64, 3)\n",
        "INTERMEDIATE_SIZE = 3072\n",
        "ATTRIBUTES_SIZE = 40\n",
        "LATENT_DIM = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry2QzPDZpn0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "gcs_pattern = 'gs://celeba-test/tfrecord_*.tfrec'\n",
        "\n",
        "filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "\n",
        "def parse_attribute_list(example):\n",
        "  features = {\n",
        "      \"names\": tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  attributes_names = example['names']\n",
        "  return attributes_names\n",
        "\n",
        "def get_names():\n",
        "  record = tf.data.TFRecordDataset('gs://celeba-test/attribute_list.tfrec')\n",
        "  attributes = record.map(parse_attribute_list)\n",
        "  att_names = next(attributes.as_numpy_iterator()).decode(\"utf-8\")\n",
        "  att_names_list = [elem.strip()[1:-1] for elem in att_names.split(',')]\n",
        "  return att_names_list\n",
        "\n",
        "att_names_list = get_names()\n",
        "\n",
        "feature_dict = {\n",
        "      \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "      \"labels\": tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True),\n",
        "  }\n",
        "\n",
        "def parse_tfrecord(example):\n",
        "  features = feature_dict\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  decoded = tf.image.decode_image(example['image'])  \n",
        "  normalized = tf.cast(decoded, tf.float32) / 255.0 # convert each 0-255 value to floats in [0, 1] range\n",
        "  image_tensor = tf.reshape(normalized, [ORIGINAL_IMAGE_SIZE[0], ORIGINAL_IMAGE_SIZE[1], ORIGINAL_IMAGE_SIZE[2]])\n",
        "  image_tensor = tf.image.resize(image_tensor[45:173,25:153], (TARGET_IMAGE_SIZE[0], TARGET_IMAGE_SIZE[1])) # crop and reshape the image \n",
        "  labels = example['labels']\n",
        "  labels = tf.cast(labels,tf.float32)\n",
        "  return  {\"encoder_input\":image_tensor,\"labels\": labels}\n",
        "\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n",
        "\n",
        "\n",
        "all_dataset = load_dataset(filenames)\n",
        "\n",
        "test_dataset = all_dataset.take(50000) \n",
        "temp_dataset = all_dataset.skip(50000)\n",
        "validation_dataset = temp_dataset.take(50000)\n",
        "training_dataset = temp_dataset.skip(50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6NGjyBGdmni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
        "\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=AUTO)\n",
        "\n",
        "  return ds\n",
        "\n",
        "training_dataset = prepare_for_training(training_dataset)\n",
        "test_dataset = prepare_for_training(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aoOr26pgNup",
        "colab_type": "text"
      },
      "source": [
        "Let's take a peek at the dataset we've created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK3IHe5aXlJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(image_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "    ax = plt.subplot(5,5,n+1)\n",
        "    plt.imshow(image_batch[n])\n",
        "    plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1uFdQJqXrdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_batch = next(iter(training_dataset))\n",
        "show_batch(image_batch[\"encoder_input\"].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvs16U9cFztm",
        "colab_type": "text"
      },
      "source": [
        "## Network model\n",
        "Here, we build the network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZaGJcnMu5uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampling(Layer):\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rYrp03YvJWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vae():\n",
        "  # Define encoder model.\n",
        "  input_img = Input(shape =(64, 64, 3), name='encoder_input')\n",
        "  y = Input(shape =(40,), name='labels')\n",
        "\n",
        "  x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu', name = 'encoder_conv_0')(input_img)\n",
        "  x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu', name = 'encoder_conv_1')(x)\n",
        "  x = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu', name = 'encoder_conv_2')(x)\n",
        "  x = Conv2D(filters = 256, kernel_size = 3, strides = 2, padding = 'same', activation = 'relu', name = 'encoder_conv_3')(x)\n",
        "\n",
        "  shape_before_flattening = K.int_shape(x)[1:]\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  z_mean = Dense(LATENT_DIM, name='z_mean')(x)\n",
        "  z_log_var = Dense(LATENT_DIM, name='z_log_var')(x)\n",
        "\n",
        "\n",
        "  # Define decoder\n",
        "  z = Sampling()((z_mean, z_log_var))\n",
        "\n",
        "  zy = Concatenate()([z,y])\n",
        "\n",
        "  # To get an exact mirror image of the encoder\n",
        "  dec1 = Dense(np.prod(shape_before_flattening))(zy)\n",
        "  dec1 = Reshape(shape_before_flattening)(dec1)\n",
        "\n",
        "  dec1 = Conv2DTranspose(filters = 128, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu', name = 'decoder_conv_0')(dec1)\n",
        "  dec1 = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu', name = 'decoder_conv_1')(dec1)\n",
        "  dec1 = Conv2DTranspose(filters = 32, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu', name = 'decoder_conv_2')(dec1)\n",
        "  x_hat = Conv2DTranspose(filters = 3, kernel_size = 3, strides = 2,  padding = 'same', activation = 'sigmoid', name = 'decoder_conv_3')(dec1)\n",
        "\n",
        "\n",
        "  # Add KL divergence regularization loss.\n",
        "  rec_loss =  12288 * tf.keras.losses.binary_crossentropy(Flatten()(input_img), Flatten()(x_hat))\n",
        "  kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "  vae_loss = K.mean(rec_loss + kl_loss)\n",
        "\n",
        "  vae = Model(inputs=[input_img,y], outputs=[x_hat], name = \"vae\")\n",
        "  vae.add_loss(vae_loss)\n",
        "  return vae, shape_before_flattening\n",
        "\n",
        "vae, shape_before_flattening = create_vae()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "vae.compile(optimizer)\n",
        "\n",
        "vae.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS7xd1uxw47a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae.fit(training_dataset, steps_per_epoch=202599//BATCH_SIZE, verbose = 1, epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9G68z5xL-je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = vae.predict(test_dataset, steps= 10)\n",
        "show_batch(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVSNS9S9Mljg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a digit generator that can sample from the learned distribution\n",
        "noise = Input(shape=(LATENT_DIM,))\n",
        "label = Input(shape=(40,))\n",
        "xy = Concatenate()([noise,label])\n",
        "dec_out = Dense(np.prod(shape_before_flattening))(xy)\n",
        "dec_out = Reshape(shape_before_flattening)(dec_out)\n",
        "dec_out = Conv2DTranspose(filters = 128, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu', name = 'decoder_conv_0')(dec_out)\n",
        "dec_out = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu', name = 'decoder_conv_1')(dec_out)\n",
        "dec_out = Conv2DTranspose(filters = 32, kernel_size = 3, strides = 2,  padding = 'same', activation = 'relu', name = 'decoder_conv_2')(dec_out)\n",
        "dec_out = Conv2DTranspose(filters = 3, kernel_size = 3, strides = 2,  padding = 'same', activation = 'sigmoid', name = 'decoder_conv_3')(dec_out)\n",
        "generator = Model(inputs=[noise,label], outputs=[dec_out])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zKOSuYEN5vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_sample = np.expand_dims(np.random.normal(size=LATENT_DIM),axis=0)\n",
        "label = np.array([-1,-1,-1,-1,-1,-1,-1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
        "label = np.expand_dims(label, axis=0)\n",
        "label = tf.convert_to_tensor(label, dtype=tf.float32)\n",
        "x_decoded = generator.predict([z_sample,label])\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(x_decoded[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}