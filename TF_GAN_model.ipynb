{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UUIJxquv6OHk"},"source":["## Overview\n","\n","\n","This notebook is hosted on GitHub. To view it in its original repository, after opening the notebook, select **File > View on GitHub**.\n","\n","### TPUs\n","\n","TPUs are chips optimized for machine learning training and inference. GAN training uses a lot of compute power, so TPUs expand the range of what we can realistically accomplish with GANs.\n","\n","### CelebA Task\n","\n","In this colab we'll use TPUs to train a GAN for a more difficult task than the MNIST task. We'll use the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset to train a model to generate faces.\n","\n","### DCGAN Architecture\n","\n","Our model implements the Deep Convolutional Generative Adversarial Network (DCGAN) architecture."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C3qZKm2uCKeW"},"source":["## Instructions"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"drBSIcprCM5Q"},"source":["<h3>  &nbsp;&nbsp;Train on TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n","\n","### Steps to run this notebook\n","\n","This notebook should be run in Colaboratory. If you are viewing this from GitHub, follow the GitHub instructions. If you are viewing this from Colaboratory, you should skip to the Colaboratory instructions.\n","\n","#### Steps from GitHub\n","\n","1. Click the `Open in Colab` badge.\n","1. Run the notebook in colaboratory by following the instructions below.\n","\n","#### Steps from Colaboratory\n","\n","1. Go to `Runtime > Change runtime type`.\n","1. Click `Hardware accelerator`.\n","1. Select `TPU` and click `Save`.\n","1. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER.  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MeoJnwf8CT5J"},"source":["### Authentication"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"by1-zWuuCWsD","outputId":"edf78506-c2f0-42ec-e3ae-fbf211841bf3","executionInfo":{"status":"ok","timestamp":1588581665891,"user_tz":-120,"elapsed":1043,"user":{"displayName":"Andrea Gayed","photoUrl":"https://lh4.googleusercontent.com/-SeDiVrDvV5Q/AAAAAAAAAAI/AAAAAAAAJK8/aOuUnDiJzEs/s64/photo.jpg","userId":"09595404113154647841"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","import os\n","import time\n","\n","# Google Cloud Storage bucket for Estimator logs and storing\n","# the training dataset.\n","bucket = 'celeba-public' #@param {type:\"string\"}\n","\n","assert bucket, 'Must specify an existing GCS bucket name'\n","print('Using bucket: {}'.format(bucket))\n","\n","model_dir = 'gs://{}/{}'.format(\n","    bucket, time.strftime('tpuestimator-tfgan/%Y-%m-%d-%H-%M-%S'))\n","print('Using model dir: {}'.format(model_dir))\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n","tpu_address = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n"],"execution_count":130,"outputs":[{"output_type":"stream","text":["Using bucket: celeba-public\n","Using model dir: gs://celeba-public/tpuestimator-tfgan/2020-05-04-08-41-06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wiV9VpWKbnLN"},"source":["### Check imports"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nXDNWK1BbpJm","colab":{}},"source":["# Check that imports for the rest of the file work.\n","!pip install tensorflow-gan\n","import tensorflow_gan as tfgan\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# Allow matplotlib images to render immediately.\n","%matplotlib inline\n","# Disable noisy outputs.\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.autograph.set_verbosity(0, False)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w7GkdNB6QB7b"},"source":["## Train and evaluate a GAN model on TPU using TF-GAN.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PoVOsRlPCDP8"},"source":["### Input pipeline\n"]},{"cell_type":"markdown","metadata":{"id":"ML9-fuMfnOhO","colab_type":"text"},"source":["Our input data is stored on Google Cloud Storage. To more fully use the parallelism TPUs offer us, and to avoid bottlenecking on data transfer, we've stored our input data in TFRecord files, 2025 images per file.\n","\n","Below, we make heavy use of `tf.data.experimental.AUTOTUNE` to optimize different parts of input loading."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LtAVr-4CP1rp","colab":{}},"source":["AUTO = tf.data.experimental.AUTOTUNE\n","\n","gcs_pattern = 'gs://celeba-public/tfrecord_*.tfrec'\n","\n","filenames = tf.io.gfile.glob(gcs_pattern)\n","\n","GENERATE_RES = 1 # Generation resolution factor (1=32, 2=64, 3=96, 4=128, etc.)\n","\n","GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n","\n","IMAGE_CHANNELS = 3\n","\n","IMAGE_SIZE = (GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n","\n","def parse_attribute_list(example):\n","  features = {\n","      \"names\": tf.io.FixedLenFeature([], tf.string),\n","  }\n","\n","  example = tf.io.parse_single_example(example, features)\n","  attributes_names = example['names']\n","  return attributes_names\n","\n","def get_names():\n","  record = tf.data.TFRecordDataset('gs://celeba-test/attribute_list.tfrec')\n","  attributes = record.map(parse_attribute_list)\n","  att_names = next(attributes.as_numpy_iterator()).decode(\"utf-8\")\n","  att_names_list = [elem.strip()[1:-1] for elem in att_names.split(',')]\n","  return att_names_list\n","\n","att_names_list = get_names()\n","\n","def input_fn(mode, params):\n","  assert 'batch_size' in params\n","  assert 'noise_dims' in params\n","  bs = params['batch_size']\n","  nd = params['noise_dims']\n","  shuffle = (mode == tf.estimator.ModeKeys.TRAIN)\n","  just_noise = (mode == tf.estimator.ModeKeys.PREDICT)\n","  \n","  lambda_noise = lambda _: tf.random.normal([bs, nd])\n","\n","  noise_ds = (tf.data.Dataset.from_tensors(0)\n","              .map(lambda_noise)\n","              # If 'predict', just generate one batch.\n","              .repeat(1 if just_noise else None))\n","  if just_noise:\n","    return noise_ds\n","\n","\n","  feature_dict = {\n","        \"filename\": tf.io.FixedLenFeature([], tf.string),\n","        \"height\": tf.io.FixedLenFeature([], tf.int64),\n","        \"width\": tf.io.FixedLenFeature([], tf.int64),\n","        \"depth\": tf.io.FixedLenFeature([], tf.int64),\n","        \"image\": tf.io.FixedLenFeature([], tf.string),\n","    }\n","\n","  attributes_dict = dict(zip(att_names_list, [tf.io.FixedLenFeature([], tf.int64) for elem in att_names_list]))\n","\n","  feature_dict.update(attributes_dict) \n","\n","  def parse_tfrecord(example):\n","    features = feature_dict\n","    example = tf.io.parse_single_example(example, features)\n","    #filename = example['filename']\n","    width = tf.cast(example['width'],tf.int64)\n","    height = tf.cast(example['height'],tf.int64)\n","    decoded = tf.image.decode_image(example['image'])  \n","    normalized = tf.cast(decoded, tf.float32) / 255.0 # convert each 0-255 value to floats in [0, 1] range\n","    image_tensor = tf.reshape(normalized, [height, width, 3])\n","    image_tensor = tf.image.resize(image_tensor[45:173,25:153], (IMAGE_SIZE[0], IMAGE_SIZE[1])) # crop and reshape the image \n","    #attr_dict = {}\n","    #for name in att_names_list:\n","    #  attr_dict[name] = example[name]\n","\n","    #Â return filename, image_tensor, attr_dict\n","    return image_tensor\n","\n","  def load_dataset(filenames):\n","    # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n","    records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n","    return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n","\n","  images_ds = load_dataset(filenames).cache().repeat()\n","  \n","  if shuffle:\n","    images_ds = images_ds.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n","    \n","  images_ds = (images_ds.batch(bs, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n","\n","  return tf.data.Dataset.zip((noise_ds, images_ds))\n","\n","def noise_input_fn(params):\n","  np.random.seed(0)\n","  np_noise = np.random.randn(params['batch_size'], params['noise_dims'])\n","  return tf.data.Dataset.from_tensors(tf.constant(np_noise, dtype=tf.float32))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rW4SlJk0COjP"},"source":["Sanity check the inputs.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X9vWFknACPvF","colab":{}},"source":["import tensorflow_datasets as tfds\n","\n","params = {'batch_size': 100, 'noise_dims':64}\n","ds = input_fn(tf.estimator.ModeKeys.EVAL, params)\n","imgs = next(tfds.as_numpy(ds))[1]\n","\n","# plot a list of loaded faces\n","def plot_faces(faces, n):\n","  plt.figure(figsize=(13,13))\n","  for i in range(n * n):\n","    # define subplot\n","    plt.subplot(n, n, 1 + i)\n","    # turn off axis\n","    plt.axis('off')\n","    # plot\n","    plt.imshow(faces[i])\n","  plt.tight_layout()\n","  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n","  plt.show()\n","\n","plot_faces(imgs, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MfmvF-krJhQi"},"source":["### Neural Net Architecture\n","\n","As usual, our GAN has two separate networks:\n","\n","*  A generator that takes input noise and outputs images\n","*  A discriminator that takes images and outputs a probability of being real\n","\n","We define `discriminator()` and `generator()` builder functions that assemble these networks. In the \"Estimator\" section below we pass the `discriminator()` and `generator()` functions to the `TPUGANEstimator`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QV_qFuUwJcOb","colab":{}},"source":["def _leaky_relu(x):\n","  return tf.nn.leaky_relu(x, alpha=0.2)\n","\n","\n","def _batch_norm(x, is_training, name):\n","  return tf.compat.v1.layers.batch_normalization(\n","      x, momentum=0.9, epsilon=1e-5, training=is_training, name=name)\n","\n","\n","def _dense(x, channels, name):\n","  return tf.compat.v1.layers.dense(\n","      x, channels,\n","      kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n","      name=name)\n","\n","\n","def _conv2d(x, filters, kernel_size, stride, name):\n","  return tf.compat.v1.layers.conv2d(\n","      x, filters, [kernel_size, kernel_size],\n","      strides=[stride, stride], padding='same',\n","      kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n","      name=name)\n","\n","\n","def _deconv2d(x, filters, kernel_size, stride, name):\n","  return tf.compat.v1.layers.conv2d_transpose(\n","      x, filters, [kernel_size, kernel_size],\n","      strides=[stride, stride], padding='same',\n","      kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n","      name=name)\n","\n","\n","def discriminator(images, unused_conditioning, is_training=True,\n","                  scope='Discriminator'):\n","  \"\"\"Discriminator for CIFAR images.\n","\n","  Args:\n","    images: A Tensor of shape [batch size, width, height, channels], that can be\n","      either real or generated. It is the discriminator's goal to distinguish\n","      between the two.\n","    unused_conditioning: The TFGAN API can help with conditional GANs, which\n","      would require extra `condition` information to both the generator and the\n","      discriminator. Since this example is not conditional, we do not use this\n","      argument.\n","    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n","      norm uses the exponential moving average collected from population\n","      statistics.\n","    scope: A variable scope or string for the discriminator.\n","\n","  Returns:\n","    A 1D Tensor of shape [batch size] representing the confidence that the\n","    images are real. The output can lie in [-inf, inf], with positive values\n","    indicating high confidence that the images are real.\n","  \"\"\"\n","  with tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):\n","    x = _conv2d(images, 64, 5, 2, name='d_conv1')\n","    x = _leaky_relu(x)\n","\n","    x = _conv2d(x, 128, 5, 2, name='d_conv2')\n","    x = _leaky_relu(_batch_norm(x, is_training, name='d_bn2'))\n","\n","    x = _conv2d(x, 256, 5, 2, name='d_conv3')\n","    x = _leaky_relu(_batch_norm(x, is_training, name='d_bn3'))\n","\n","    x = tf.reshape(x, [-1, 4 * 4 * 256])\n","\n","    x = _dense(x, 1, name='d_fc_4')\n","\n","    return x\n","\n","\n","def generator(noise, is_training=True, scope='Generator'):\n","  \"\"\"Generator to produce CIFAR images.\n","\n","  Args:\n","    noise: A 2D Tensor of shape [batch size, noise dim]. Since this example\n","      does not use conditioning, this Tensor represents a noise vector of some\n","      kind that will be reshaped by the generator into CIFAR examples.\n","    is_training: If `True`, batch norm uses batch statistics. If `False`, batch\n","      norm uses the exponential moving average collected from population\n","      statistics.\n","    scope: A variable scope or string for the generator.\n","\n","  Returns:\n","    A single Tensor with a batch of generated CIFAR images.\n","  \"\"\"\n","  with tf.compat.v1.variable_scope(scope, reuse=tf.compat.v1.AUTO_REUSE):\n","    net = _dense(noise, 4096, name='g_fc1')\n","    net = tf.nn.relu(_batch_norm(net, is_training, name='g_bn1'))\n","\n","    net = tf.reshape(net, [-1, 4, 4, 256])\n","\n","    net = _deconv2d(net, 128, 5, 2, name='g_dconv2')\n","    net = tf.nn.relu(_batch_norm(net, is_training, name='g_bn2'))\n","\n","    net = _deconv2d(net, 64, 4, 2, name='g_dconv3')\n","    net = tf.nn.relu(_batch_norm(net, is_training, name='g_bn3'))\n","\n","    net = _deconv2d(net, 3, 4, 2, name='g_dconv4')\n","    net = tf.tanh(net)\n","\n","    return net"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sr4rnauKJlHu"},"source":["### Estimator\n","\n","TF-GAN's `TPUGANEstimator` is like `GANEstimator`, but it extends TensorFlow's `TPUEstimator` class. `TPUEstimator` handles the details of deploying the network on a TPU."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"iliAVH61a9y4","colab":{}},"source":["import tensorflow.compat.v1 as tf_compat\n","\n","noise_dims = 1024 #@param\n","generator_lr = 0.0002  #@param\n","discriminator_lr = 0.0002  #@param\n","train_batch_size = 1024  #@param\n","images_per_batch = 2000 #@param\n","\n","config = tf.compat.v1.estimator.tpu.RunConfig(\n","    model_dir=model_dir,\n","    master=tpu_address,\n","    tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(iterations_per_loop=images_per_batch),\n","    #save_summary_steps=None,\n","    #save_checkpoints_secs=None\n","    )\n","    \n","est = tfgan.estimator.TPUGANEstimator(\n","    generator_fn=generator,\n","    discriminator_fn=discriminator,\n","    generator_loss_fn=tfgan.losses.modified_generator_loss,\n","    discriminator_loss_fn=tfgan.losses.modified_discriminator_loss,\n","    generator_optimizer=tf.compat.v1.train.AdamOptimizer(generator_lr, 0.5),\n","    discriminator_optimizer=tf.compat.v1.train.AdamOptimizer(discriminator_lr, 0.5),\n","    joint_train=True,  # train G and D jointly instead of sequentially.\n","    train_batch_size=train_batch_size,\n","    predict_batch_size=images_per_batch,\n","    use_tpu=True,\n","    params={'noise_dims': noise_dims},\n","    config=config)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7juQranbJu_3"},"source":["### Train and Eval Loop\n","\n","Train and vizualize."]},{"cell_type":"code","metadata":{"id":"yrKs__mCYTyT","colab_type":"code","colab":{}},"source":["def hms_string(sec_elapsed):\n","  h = int(sec_elapsed / (60 * 60))\n","  m = int((sec_elapsed % (60 * 60)) / 60)\n","  s = sec_elapsed % 60\n","  return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wXEOA5aHGAG-","colab":{}},"source":["epochs = 10000 #@param\n","\n","start = time.time()\n","est.train(input_fn,steps=epochs)\n","elapsed = time.time()-start\n","print (\"Training time: {}\".format(hms_string(elapsed)))\n","\n","# Generate and show some predictions.\n","predictions = np.array([x['generated_data'] for x in est.predict(noise_input_fn)])[:100]\n","plot_faces(predictions,10)"],"execution_count":0,"outputs":[]}]}